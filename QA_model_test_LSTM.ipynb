{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('F:/QA_with_NN/here_is_your_answer/dev-v2.0.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data\n",
    "\n",
    "train_data_list = []\n",
    "context = []\n",
    "question = []\n",
    "answer = []\n",
    "answer_start = []\n",
    "is_impossible = []\n",
    "example = []\n",
    "\n",
    "for i in range(len(train['data'])):\n",
    "    for j in range(len(train['data'][i]['paragraphs'])):\n",
    "        context.append(train['data'][i]['paragraphs'][j]['context'])\n",
    "        for k in range(len(train['data'][i]['paragraphs'][j]['qas'])):\n",
    "            question.append(train['data'][i]['paragraphs'][j]['qas'][k]['question'])\n",
    "            is_impossible.append(train['data'][i]['paragraphs'][j]['qas'][k]['is_impossible'])\n",
    "            for l in range(len(train['data'][i]['paragraphs'][j]['qas'][k]['answers'])):\n",
    "                answer.append(train['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text'])\n",
    "                answer_start.append(train['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['answer_start'])\n",
    "               \n",
    "\n",
    "for i in range(len(train['data'])):\n",
    "    for j in range(len(train['data'][i]['paragraphs'])):\n",
    "        context_1 = train['data'][i]['paragraphs'][j]['context']\n",
    "        for k in range(len(train['data'][i]['paragraphs'][j]['qas'])):\n",
    "            question_1 = train['data'][i]['paragraphs'][j]['qas'][k]['question']\n",
    "            is_impossible_1 = train['data'][i]['paragraphs'][j]['qas'][k]['is_impossible']\n",
    "            if is_impossible_1 == False:\n",
    "                for l in range(len(train['data'][i]['paragraphs'][j]['qas'][k]['answers'])):\n",
    "                    answer_1 = train['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text']\n",
    "                    answer_start_1 = train['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['answer_start']\n",
    "            else:  \n",
    "                answer_1 = \"\"\n",
    "                answer_start_1= \"\"\n",
    "            example.append((context_1, question_1, is_impossible_1, answer_1, answer_start_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n",
       " 'When were the Normans in Normandy?',\n",
       " False,\n",
       " '10th and 11th centuries',\n",
       " 94)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[1]\n",
    "#  context, question, is_impossible, answer, answer_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11873"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking only on 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>False</td>\n",
       "      <td>France</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>When were the Normans in Normandy?</td>\n",
       "      <td>False</td>\n",
       "      <td>10th and 11th centuries</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>From which countries did the Norse originate?</td>\n",
       "      <td>False</td>\n",
       "      <td>Denmark, Iceland and Norway</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...   \n",
       "2  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                                        question  is_impossible  \\\n",
       "0           In what country is Normandy located?          False   \n",
       "1             When were the Normans in Normandy?          False   \n",
       "2  From which countries did the Norse originate?          False   \n",
       "\n",
       "                        answer answer_start  \n",
       "0                       France          159  \n",
       "1      10th and 11th centuries           94  \n",
       "2  Denmark, Iceland and Norway          256  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(example)\n",
    "df = dataframe.head(3)\n",
    "df.columns = ['context', 'question', 'is_impossible', 'answer', 'answer_start']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create vocab file, tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go through every file (train and validaton: context and question) and then get distinct words saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bijno\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize_text(phrase):\n",
    "    t  = Tokenizer()\n",
    "\n",
    "    t.fit_on_texts(phrase)\n",
    "\n",
    "    sequences = t.texts_to_sequences(phrase)\n",
    "    \n",
    "    inverse_word_index= {value: key for key, value in t.word_index.items()}\n",
    "\n",
    "    return list(inverse_word_index.values()), t.word_index, inverse_word_index, sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'in', 'from', 'their', 'normans', 'were', 'norman', 'who', '10th', 'centuries', 'west', 'francia', 'with', 'normandy', 'norse', 'nourmands', 'french', 'normands', 'latin', 'normanni', 'people', '11th', 'gave', 'name', 'a', 'region', 'france', 'they', 'descended', 'comes', 'norseman', 'raiders', 'pirates', 'denmark', 'iceland', 'norway', 'under', 'leader', 'rollo', 'agreed', 'swear', 'fealty', 'king', 'charles', 'iii', 'through', 'generations', 'assimilation', 'mixing', 'native', 'frankish', 'roman', 'gaulish', 'populations', 'descendants', 'would', 'gradually', 'merge', 'carolingian', 'based', 'cultures', 'distinct', 'cultural', 'ethnic', 'identity', 'emerged', 'initially', 'first', 'half', 'century', 'it', 'continued', 'evolve', 'over', 'succeeding', 'what', 'country', 'is', 'located', 'when', 'which', 'countries', 'did', 'originate']\n",
      "~~~~~\n",
      "[[1, 8, 10, 19, 20, 21, 22, 23, 9, 1, 24, 11, 5, 1, 12, 2, 25, 13, 26, 7, 27, 4, 17, 28, 29, 5, 30, 31, 9, 32, 6, 18, 10, 33, 6, 34, 35, 2, 36, 6, 37, 38, 2, 39, 11, 40, 7, 41, 42, 43, 4, 44, 45, 4, 46, 47, 48, 3, 14, 15, 49, 50, 3, 51, 2, 52, 16, 1, 53, 54, 2, 55, 56, 57, 7, 58, 59, 60, 61, 16, 1, 62, 63, 64, 3, 14, 15, 1, 65, 66, 2, 67, 68, 3, 1, 8, 69, 70, 5, 1, 71, 72, 3, 1, 12, 73, 2, 74, 75, 4, 76, 77, 1, 78, 13], [5, 79, 80, 81, 17, 82], [1, 8, 10, 19, 20, 21, 22, 23, 9, 1, 24, 11, 5, 1, 12, 2, 25, 13, 26, 7, 27, 4, 17, 28, 29, 5, 30, 31, 9, 32, 6, 18, 10, 33, 6, 34, 35, 2, 36, 6, 37, 38, 2, 39, 11, 40, 7, 41, 42, 43, 4, 44, 45, 4, 46, 47, 48, 3, 14, 15, 49, 50, 3, 51, 2, 52, 16, 1, 53, 54, 2, 55, 56, 57, 7, 58, 59, 60, 61, 16, 1, 62, 63, 64, 3, 14, 15, 1, 65, 66, 2, 67, 68, 3, 1, 8, 69, 70, 5, 1, 71, 72, 3, 1, 12, 73, 2, 74, 75, 4, 76, 77, 1, 78, 13], [83, 9, 1, 8, 5, 17], [1, 8, 10, 19, 20, 21, 22, 23, 9, 1, 24, 11, 5, 1, 12, 2, 25, 13, 26, 7, 27, 4, 17, 28, 29, 5, 30, 31, 9, 32, 6, 18, 10, 33, 6, 34, 35, 2, 36, 6, 37, 38, 2, 39, 11, 40, 7, 41, 42, 43, 4, 44, 45, 4, 46, 47, 48, 3, 14, 15, 49, 50, 3, 51, 2, 52, 16, 1, 53, 54, 2, 55, 56, 57, 7, 58, 59, 60, 61, 16, 1, 62, 63, 64, 3, 14, 15, 1, 65, 66, 2, 67, 68, 3, 1, 8, 69, 70, 5, 1, 71, 72, 3, 1, 12, 73, 2, 74, 75, 4, 76, 77, 1, 78, 13], [6, 84, 85, 86, 1, 18, 87]]\n",
      "~~~~~\n",
      "87\n",
      "~~~~~\n",
      "{1: 'the', 2: 'and', 3: 'of', 4: 'to', 5: 'in', 6: 'from', 7: 'their', 8: 'normans', 9: 'were', 10: 'norman', 11: 'who', 12: '10th', 13: 'centuries', 14: 'west', 15: 'francia', 16: 'with', 17: 'normandy', 18: 'norse', 19: 'nourmands', 20: 'french', 21: 'normands', 22: 'latin', 23: 'normanni', 24: 'people', 25: '11th', 26: 'gave', 27: 'name', 28: 'a', 29: 'region', 30: 'france', 31: 'they', 32: 'descended', 33: 'comes', 34: 'norseman', 35: 'raiders', 36: 'pirates', 37: 'denmark', 38: 'iceland', 39: 'norway', 40: 'under', 41: 'leader', 42: 'rollo', 43: 'agreed', 44: 'swear', 45: 'fealty', 46: 'king', 47: 'charles', 48: 'iii', 49: 'through', 50: 'generations', 51: 'assimilation', 52: 'mixing', 53: 'native', 54: 'frankish', 55: 'roman', 56: 'gaulish', 57: 'populations', 58: 'descendants', 59: 'would', 60: 'gradually', 61: 'merge', 62: 'carolingian', 63: 'based', 64: 'cultures', 65: 'distinct', 66: 'cultural', 67: 'ethnic', 68: 'identity', 69: 'emerged', 70: 'initially', 71: 'first', 72: 'half', 73: 'century', 74: 'it', 75: 'continued', 76: 'evolve', 77: 'over', 78: 'succeeding', 79: 'what', 80: 'country', 81: 'is', 82: 'located', 83: 'when', 84: 'which', 85: 'countries', 86: 'did', 87: 'originate'}\n",
      "~~~~~\n",
      "{'the': 1, 'and': 2, 'of': 3, 'to': 4, 'in': 5, 'from': 6, 'their': 7, 'normans': 8, 'were': 9, 'norman': 10, 'who': 11, '10th': 12, 'centuries': 13, 'west': 14, 'francia': 15, 'with': 16, 'normandy': 17, 'norse': 18, 'nourmands': 19, 'french': 20, 'normands': 21, 'latin': 22, 'normanni': 23, 'people': 24, '11th': 25, 'gave': 26, 'name': 27, 'a': 28, 'region': 29, 'france': 30, 'they': 31, 'descended': 32, 'comes': 33, 'norseman': 34, 'raiders': 35, 'pirates': 36, 'denmark': 37, 'iceland': 38, 'norway': 39, 'under': 40, 'leader': 41, 'rollo': 42, 'agreed': 43, 'swear': 44, 'fealty': 45, 'king': 46, 'charles': 47, 'iii': 48, 'through': 49, 'generations': 50, 'assimilation': 51, 'mixing': 52, 'native': 53, 'frankish': 54, 'roman': 55, 'gaulish': 56, 'populations': 57, 'descendants': 58, 'would': 59, 'gradually': 60, 'merge': 61, 'carolingian': 62, 'based': 63, 'cultures': 64, 'distinct': 65, 'cultural': 66, 'ethnic': 67, 'identity': 68, 'emerged': 69, 'initially': 70, 'first': 71, 'half': 72, 'century': 73, 'it': 74, 'continued': 75, 'evolve': 76, 'over': 77, 'succeeding': 78, 'what': 79, 'country': 80, 'is': 81, 'located': 82, 'when': 83, 'which': 84, 'countries': 85, 'did': 86, 'originate': 87}\n"
     ]
    }
   ],
   "source": [
    "def create_word_vocab(df):\n",
    "    sentences_list = []\n",
    "\n",
    "    for eachRow in df.itertuples():\n",
    "        sentences_list.append(eachRow.context)\n",
    "        sentences_list.append(eachRow.question)\n",
    "\n",
    "    return sentences_list\n",
    "\n",
    "vocab_tokens, word_index, inverse_word_index, sequence = tokenize_text(create_word_vocab(df))\n",
    "print(vocab_tokens)\n",
    "print('~~~~~')\n",
    "print(sequence)\n",
    "print('~~~~~')\n",
    "print(len(word_index))\n",
    "print('~~~~~')\n",
    "print(inverse_word_index)\n",
    "print('~~~~~')\n",
    "print(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "\n",
    "LENGTH_CONTEXT = 200\n",
    "LENGTH_QUESTION=10\n",
    "LENGTH_ANSWER=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "def sequence_list(phrase, maxlen, data=df, word_index=word_index):\n",
    "    seq_list=[]\n",
    "    token_list = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        t, _, _, _ = tokenize_text([row[phrase]])\n",
    "        token_list.append(t)\n",
    "        seq_list.append([word_index[w] for w in t])\n",
    "\n",
    "    padded_data = pad_sequences(seq_list, maxlen=maxlen)\n",
    "\n",
    "    return token_list, seq_list, padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['france'], ['10th', 'and', '11th', 'centuries'], ['denmark', 'iceland', 'and', 'norway']]\n",
      "~~~~~~~~\n",
      "[[5, 79, 80, 81, 17, 82], [83, 9, 1, 8, 5, 17], [6, 84, 85, 86, 1, 18, 87]]\n",
      "~~~~~~~~\n",
      "[[ 0  0  0  0  5 79 80 81 17 82]\n",
      " [ 0  0  0  0 83  9  1  8  5 17]\n",
      " [ 0  0  0  6 84 85 86  1 18 87]]\n",
      "~~~~~~~~\n",
      "[[1, 2, 3, 4, 5, 7, 6, 8, 10, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28, 29, 30, 31, 32, 18, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78], [1, 2, 3, 4, 5, 7, 6, 8, 10, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28, 29, 30, 31, 32, 18, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78], [1, 2, 3, 4, 5, 7, 6, 8, 10, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28, 29, 30, 31, 32, 18, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]]\n",
      "~~~~~~~~\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1  2  3  4  5  7  6  8 10  9 11 12 13 14 15 16 19 20 21 22 23 24\n",
      "  25 26 27 17 28 29 30 31 32 18 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      "  47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      "  71 72 73 74 75 76 77 78]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1  2  3  4  5  7  6  8 10  9 11 12 13 14 15 16 19 20 21 22 23 24\n",
      "  25 26 27 17 28 29 30 31 32 18 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      "  47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      "  71 72 73 74 75 76 77 78]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1  2  3  4  5  7  6  8 10  9 11 12 13 14 15 16 19 20 21 22 23 24\n",
      "  25 26 27 17 28 29 30 31 32 18 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      "  47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      "  71 72 73 74 75 76 77 78]]\n",
      "~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "tokenizer_answer, seq_list_answer, padded_data_answer = sequence_list(phrase = 'answer', maxlen = LENGTH_ANSWER)\n",
    "\n",
    "print(tokenizer_answer)\n",
    "print('~~~~~~~~')\n",
    "\n",
    "tokenizer_question, seq_list_question, padded_data_question = sequence_list(phrase = 'question', maxlen = LENGTH_QUESTION)\n",
    "\n",
    "print(seq_list_question)\n",
    "print('~~~~~~~~')\n",
    "print(padded_data_question)\n",
    "print('~~~~~~~~')\n",
    "\n",
    "\n",
    "tokenizer_context, seq_list_context, padded_data_context = sequence_list(phrase = 'context', maxlen = LENGTH_CONTEXT)\n",
    "\n",
    "print(seq_list_context)\n",
    "print('~~~~~~~~')\n",
    "print(padded_data_context)\n",
    "print('~~~~~~~~')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence answer array from the vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france\n",
      "10th\n",
      "and\n",
      "11th\n",
      "centuries\n",
      "denmark\n",
      "iceland\n",
      "and\n",
      "norway\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ans_arr=[]\n",
    "for answers in tokenizer_answer:\n",
    "    y = np.zeros(len(word_index) + 1)\n",
    "    for item in answers:\n",
    "        print(item)\n",
    "        y[word_index[item]] = 1\n",
    "    ans_arr.append(y)\n",
    "\n",
    "print(ans_arr)\n",
    "\n",
    "pred_arr = np.array(ans_arr)\n",
    "\n",
    "pred_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bijno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Embedding, RepeatVector, Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "sentence_passage = Input(shape=(LENGTH_CONTEXT,), dtype='int32')\n",
    "encoded_sentence_passage = Embedding(len(word_index)+1, hidden_size)(sentence_passage)\n",
    "\n",
    "question = Input(shape=(LENGTH_QUESTION,), dtype='int32')\n",
    "encoded_question = Embedding(len(word_index)+1, hidden_size)(question)\n",
    "encoded_question = LSTM(hidden_size)(encoded_question)\n",
    "encoded_question = RepeatVector(LENGTH_CONTEXT)(encoded_question)\n",
    "\n",
    "merge = layers.add([encoded_sentence_passage, encoded_question])\n",
    "merge = LSTM(hidden_size,go_backwards=True)(merge)\n",
    "predicted = Dense(len(inverse_word_index)+1, activation='softmax')(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bijno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 2s 527ms/step - loss: 13.4551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e5b9288390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo\n",
    "model = Model([sentence_passage, question], predicted)\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', )\n",
    "model.fit( [padded_data_context, padded_data_question], pred_arr, batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>What century did the Normans first gain their ...</td>\n",
       "      <td>False</td>\n",
       "      <td>10th</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "4  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                                            question is_impossible answer  \\\n",
       "4  What century did the Normans first gain their ...         False   10th   \n",
       "\n",
       "  answer_start  \n",
       "4          671  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(dataframe.iloc[4]).T\n",
    "df_test.columns = ['context', 'question', 'is_impossible', 'answer', 'answer_start']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'of': 3,\n",
       " 'their': 4,\n",
       " 'to': 5,\n",
       " 'normans': 6,\n",
       " 'in': 7,\n",
       " 'from': 8,\n",
       " 'norman': 9,\n",
       " 'were': 10,\n",
       " 'who': 11,\n",
       " '10th': 12,\n",
       " 'centuries': 13,\n",
       " 'west': 14,\n",
       " 'francia': 15,\n",
       " 'with': 16,\n",
       " 'identity': 17,\n",
       " 'first': 18,\n",
       " 'century': 19,\n",
       " 'nourmands': 20,\n",
       " 'french': 21,\n",
       " 'normands': 22,\n",
       " 'latin': 23,\n",
       " 'normanni': 24,\n",
       " 'people': 25,\n",
       " '11th': 26,\n",
       " 'gave': 27,\n",
       " 'name': 28,\n",
       " 'normandy': 29,\n",
       " 'a': 30,\n",
       " 'region': 31,\n",
       " 'france': 32,\n",
       " 'they': 33,\n",
       " 'descended': 34,\n",
       " 'norse': 35,\n",
       " 'comes': 36,\n",
       " 'norseman': 37,\n",
       " 'raiders': 38,\n",
       " 'pirates': 39,\n",
       " 'denmark': 40,\n",
       " 'iceland': 41,\n",
       " 'norway': 42,\n",
       " 'under': 43,\n",
       " 'leader': 44,\n",
       " 'rollo': 45,\n",
       " 'agreed': 46,\n",
       " 'swear': 47,\n",
       " 'fealty': 48,\n",
       " 'king': 49,\n",
       " 'charles': 50,\n",
       " 'iii': 51,\n",
       " 'through': 52,\n",
       " 'generations': 53,\n",
       " 'assimilation': 54,\n",
       " 'mixing': 55,\n",
       " 'native': 56,\n",
       " 'frankish': 57,\n",
       " 'roman': 58,\n",
       " 'gaulish': 59,\n",
       " 'populations': 60,\n",
       " 'descendants': 61,\n",
       " 'would': 62,\n",
       " 'gradually': 63,\n",
       " 'merge': 64,\n",
       " 'carolingian': 65,\n",
       " 'based': 66,\n",
       " 'cultures': 67,\n",
       " 'distinct': 68,\n",
       " 'cultural': 69,\n",
       " 'ethnic': 70,\n",
       " 'emerged': 71,\n",
       " 'initially': 72,\n",
       " 'half': 73,\n",
       " 'it': 74,\n",
       " 'continued': 75,\n",
       " 'evolve': 76,\n",
       " 'over': 77,\n",
       " 'succeeding': 78,\n",
       " 'what': 79,\n",
       " 'did': 80,\n",
       " 'gain': 81,\n",
       " 'separate': 82}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokens_test, word_index_test, inverse_word_index_test, sequence_test = tokenize_text(create_word_vocab(df_test))\n",
    "\n",
    "word_index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 79, 80, 81, 17, 82], [83, 9, 1, 8, 5, 17], [6, 84, 85, 86, 1, 18, 87]]\n",
      "~~~~~~~~\n",
      "[[ 0  0  0  0  5 79 80 81 17 82]\n",
      " [ 0  0  0  0 83  9  1  8  5 17]\n",
      " [ 0  0  0  6 84 85 86  1 18 87]]\n",
      "~~~~~~~~\n",
      "[[1, 2, 3, 4, 5, 7, 6, 8, 10, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28, 29, 30, 31, 32, 18, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78], [1, 2, 3, 4, 5, 7, 6, 8, 10, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28, 29, 30, 31, 32, 18, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78], [1, 2, 3, 4, 5, 7, 6, 8, 10, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28, 29, 30, 31, 32, 18, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]]\n",
      "~~~~~~~~\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1  2  3  4  5  7  6  8 10  9 11 12 13 14 15 16 19 20 21 22 23 24\n",
      "  25 26 27 17 28 29 30 31 32 18 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      "  47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      "  71 72 73 74 75 76 77 78]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1  2  3  4  5  7  6  8 10  9 11 12 13 14 15 16 19 20 21 22 23 24\n",
      "  25 26 27 17 28 29 30 31 32 18 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      "  47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      "  71 72 73 74 75 76 77 78]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1  2  3  4  5  7  6  8 10  9 11 12 13 14 15 16 19 20 21 22 23 24\n",
      "  25 26 27 17 28 29 30 31 32 18 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      "  47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n",
      "  71 72 73 74 75 76 77 78]]\n",
      "~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "_, _, padded_data_question_test = sequence_list(phrase = 'question', maxlen = LENGTH_QUESTION, data = df_test, word_index = word_index_test)\n",
    "\n",
    "print(seq_list_question)\n",
    "print('~~~~~~~~')\n",
    "print(padded_data_question)\n",
    "print('~~~~~~~~')\n",
    "\n",
    "_, _, padded_data_context_test = sequence_list(phrase = 'context', maxlen = LENGTH_CONTEXT, data = df_test, word_index = word_index_test)\n",
    "\n",
    "print(seq_list_context)\n",
    "print('~~~~~~~~')\n",
    "print(padded_data_context)\n",
    "print('~~~~~~~~')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicts the probability of answer from the word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01133863 0.0109193  0.01144018 0.01095803 0.01138768 0.01113397\n",
      "  0.01149548 0.01117544 0.01145334 0.01162995 0.01158531 0.01122922\n",
      "  0.01175249 0.01170729 0.01123376 0.01103042 0.01141837 0.01121611\n",
      "  0.01120491 0.01150394 0.01107252 0.01166896 0.01137938 0.01129903\n",
      "  0.01166917 0.01168458 0.01139489 0.0110339  0.01134506 0.01140233\n",
      "  0.01136044 0.01143155 0.01109564 0.01162125 0.01150065 0.01129172\n",
      "  0.01130632 0.01159576 0.01146255 0.01160004 0.01134275 0.01139465\n",
      "  0.01144834 0.01159172 0.0110982  0.01160903 0.01149249 0.01131669\n",
      "  0.01143707 0.01089211 0.01110673 0.01160846 0.01140953 0.01123598\n",
      "  0.01147363 0.01152219 0.01129311 0.01103824 0.01142149 0.01106279\n",
      "  0.01148623 0.01123265 0.01128522 0.01133939 0.01132272 0.01106387\n",
      "  0.01127423 0.01134555 0.01152323 0.01149854 0.01162649 0.01113039\n",
      "  0.01153156 0.01110622 0.01127761 0.01156342 0.01167452 0.01148697\n",
      "  0.01127249 0.01156499 0.01152734 0.01142406 0.01121476 0.01135552\n",
      "  0.01110801 0.01107397 0.01136229 0.01150295]] 88 <class 'numpy.ndarray'>\n",
      "~~~~`\n",
      "prediction:  10th\n"
     ]
    }
   ],
   "source": [
    "ans=model.predict([padded_data_context_test,padded_data_question_test])\n",
    "\n",
    "print(ans, len(ans[0]), type(ans))\n",
    "print('~~~~`')\n",
    "print(\"prediction: \", inverse_word_index_test.get(ans.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
